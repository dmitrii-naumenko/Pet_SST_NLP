{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import imblearn as ib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import transformers\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv('../data/valid.csv')\n",
    "df_train = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_ru</th>\n",
       "      <th>speech_fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>706</td>\n",
       "      <td>706</td>\n",
       "      <td>Watching Xela firefighters struggle to save bu...</td>\n",
       "      <td>1</td>\n",
       "      <td>Наблюдая за тем, как пожарные Xela борются за ...</td>\n",
       "      <td>706.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>707</td>\n",
       "      <td>707</td>\n",
       "      <td>Gates not body bagging nobody???????? niggas i...</td>\n",
       "      <td>0</td>\n",
       "      <td>Гейтс никого не запихивает в мешки с трупами??...</td>\n",
       "      <td>707.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272</td>\n",
       "      <td>272</td>\n",
       "      <td>Firefigthers Evacuate from Northampton Townshi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Пожарные эвакуируются из-за пожара в доме в Но...</td>\n",
       "      <td>272.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "      <td>FAAN orders evacuation of abandoned aircraft a...</td>\n",
       "      <td>1</td>\n",
       "      <td>FAA отдает приказ об эвакуации брошенных самол...</td>\n",
       "      <td>479.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>781</td>\n",
       "      <td>781</td>\n",
       "      <td>If you're reading this go accidentally fall of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Если ты читаешь это, то можешь случайно упасть...</td>\n",
       "      <td>781.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0           706         706   \n",
       "1           707         707   \n",
       "2           272         272   \n",
       "3           479         479   \n",
       "4           781         781   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  Watching Xela firefighters struggle to save bu...       1   \n",
       "1  Gates not body bagging nobody???????? niggas i...       0   \n",
       "2  Firefigthers Evacuate from Northampton Townshi...       1   \n",
       "3  FAAN orders evacuation of abandoned aircraft a...       1   \n",
       "4  If you're reading this go accidentally fall of...       0   \n",
       "\n",
       "                                             text_ru speech_fn  \n",
       "0  Наблюдая за тем, как пожарные Xela борются за ...   706.mp3  \n",
       "1  Гейтс никого не запихивает в мешки с трупами??...   707.mp3  \n",
       "2  Пожарные эвакуируются из-за пожара в доме в Но...   272.mp3  \n",
       "3  FAA отдает приказ об эвакуации брошенных самол...   479.mp3  \n",
       "4  Если ты читаешь это, то можешь случайно упасть...   781.mp3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/8sd3p8197w77vtz4_ts53h4m0000gn/T/ipykernel_7473/981470628.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.text = df.text.str.replace('\\n', ' ')\n",
      "/var/folders/0t/8sd3p8197w77vtz4_ts53h4m0000gn/T/ipykernel_7473/981470628.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.text = df.text.str.replace('\\n', ' ')\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df: pd.DataFrame, col: str = 'text_ru') -> pd.DataFrame:\n",
    "    df = df[[col, 'target']]\n",
    "    df.columns = ['text', 'target']\n",
    "    df.text = df.text.str.replace('\\n', ' ')\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = preprocess(df_train)\n",
    "df_valid = preprocess(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "\n",
    "re_arr = [\n",
    "    re.compile(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'),\n",
    "    re.compile(r'<.*?>'),\n",
    "    re.compile(r'[%s]' % re.escape(string.punctuation)),\n",
    "    re.compile(r'\\s+'),\n",
    "    re.compile(r'\\[[0-9]*\\]'),\n",
    "    re.compile(r'[^\\w\\s]'),\n",
    "    re.compile(r'\\d'),\n",
    "]\n",
    "\n",
    "def simplify(s: str) -> str:\n",
    "    x = s.lower().replace('\\xa0', ' ').strip()\n",
    "    for r in re_arr:\n",
    "        x = r.sub(' ', x)\n",
    "    return x\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    lemmas = [x.strip() for x in m.lemmatize(text) if x.strip() != '']\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def simplify_and_lemmatize(text: str) -> str:\n",
    "    return lemmatize(simplify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['lemm'] = df_train.text.apply(simplify_and_lemmatize)\n",
    "df_valid['lemm'] = df_valid.text.apply(simplify_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dima/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')).union(stopwords.words('russian'))\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountModel():\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vect = CountVectorizer(stop_words=stop_words) \n",
    "        self.model = LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "        self.pipe = ib.pipeline.Pipeline([\n",
    "            ('vect', self.vect),       \n",
    "            ('model', self.model)\n",
    "        ])\n",
    "        \n",
    "        self.pipe.fit(X, y)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.vect, self.model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CountModel()\n",
    "model.fit(df_train.lemm, df_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict(df_valid.lemm)\n",
    "accuracy_score(df_valid.target, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfModel():\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vect = TfidfVectorizer(stop_words=stop_words)\n",
    "        self.model = LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "        self.pipe = ib.pipeline.Pipeline([\n",
    "            ('vect', self.vect),\n",
    "            ('model', self.model)\n",
    "        ])\n",
    "        \n",
    "        self.pipe.fit(X, y)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.vect, self.model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CountModel()\n",
    "model.fit(df_train.lemm, df_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict(df_valid.lemm)\n",
    "accuracy_score(df_valid.target, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_name)\n",
    "bert = transformers.BertModel.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.labels = y\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in X]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, len(np.unique(df_train.target)))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train(model, X_train, y_train, X_valid, y_valid, learning_rate, epochs):\n",
    "\n",
    "    #X_train = X_train.head(100)\n",
    "    #y_train = y_train[:100]\n",
    "    #X_valid = X_valid.head(100)\n",
    "    #y_valid = y_valid[:100]\n",
    "\n",
    "    train, val = Dataset(X_train, y_train), Dataset(X_valid, y_valid)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(y_train): .3f} | Train Accuracy: {total_acc_train / len(y_train): .3f} | Val Loss: {total_loss_val / len(y_valid): .3f} | Val Accuracy: {total_acc_val / len(y_valid): .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def evaluate(model, X_test, y_test):\n",
    "\n",
    "    #X_test = X_test.head(1000)\n",
    "    #y_test = y_test[:1000]\n",
    "\n",
    "    test = Dataset(X_test, y_test)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              result.append(output.argmax().cpu().detach().numpy().item())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297bbbd30d264c2ead1557e78789df4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.344 | Train Accuracy:  0.527 | Val Loss:  0.335 | Val Accuracy:  0.565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ac22efbe684ff78f569db24fbb6d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.328 | Train Accuracy:  0.610 | Val Loss:  0.319 | Val Accuracy:  0.645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912ad5886b5c4362bdd5dfcf25c8e0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.306 | Train Accuracy:  0.693 | Val Loss:  0.291 | Val Accuracy:  0.695\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9a23773ded42d88b00028f1eae0961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.282 | Train Accuracy:  0.735 | Val Loss:  0.264 | Val Accuracy:  0.750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634f47b803ad4596a1f973467358b140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.245 | Train Accuracy:  0.802 | Val Loss:  0.249 | Val Accuracy:  0.775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2f501521f441be9525fb48b794c5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.216 | Train Accuracy:  0.820 | Val Loss:  0.237 | Val Accuracy:  0.805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efa2d9150c946a9b80e1bb711cc18ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.193 | Train Accuracy:  0.860 | Val Loss:  0.222 | Val Accuracy:  0.805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34605422f1b46129bf0afa25b056f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.170 | Train Accuracy:  0.870 | Val Loss:  0.217 | Val Accuracy:  0.830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b276831420074832a5fce819ec2ac219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.147 | Train Accuracy:  0.902 | Val Loss:  0.223 | Val Accuracy:  0.825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eed40668344e0982df39f26b2a1fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.123 | Train Accuracy:  0.920 | Val Loss:  0.224 | Val Accuracy:  0.815\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "EPOCHS = 10\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train.text, df_train.target, df_valid.text, df_valid.target, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "В отличии от реальных данных, в пробной тренировочной выборке дисбаланс классов не выражен. Для простоты мы использовали метрику Accuracy, хотя по дизайну должна быть Recall@Precision=99,5%\n",
    "\n",
    "- 0.77 - CountVectorizer, TfIdfVectorizer\n",
    "- **0.83** - Fine tuned BERT. Можно получить значительно лучше точность, но для большой модели нужно больше данных\n",
    "\n",
    "Вцелом значимо увеличить качество можно за счет хорошо подготовленных данных для обучения, используемых в реальной задаче.\n",
    "\n",
    "Для чистоты эксперимента, следовало бы проверить связку STT+NLP. Наше решение относится к проверке концепции, с чем успешно справилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
